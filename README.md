# TCDD Railway Maintenance AI System

This project implements an AI-powered system for railway maintenance guidance based on Transformer architectures and Reinforcement Learning (RL). 
It processes maintenance records, encodes data into tokens, trains Transformer-based classifiers, and fine-tunes the models with RL to improve fault detection and prioritization.

## Project Structure

### 1. `model.py`
- Defines the **TransformerClassifier** architecture.
- Includes token embeddings, positional encodings, multi-head attention, feed-forward layers, and transformer encoder blocks.
- Final output is classification logits for fault types (labels).
- Supports different pooling strategies ("mean" or "cls").

### 2. `prepare_dataset.py`
- Reads Excel source files:
  - Maintenance records: `Vagon_cleaned.xlsx`
  - Vocabulary source: `Vagon_analysis_output.xlsx`
- Builds a vocabulary (`vocab_mapping.json`) from categorical data across multiple sheets.
- Encodes the dataset into sequences of token IDs, adding priority tokens based on component and label frequency.
- Outputs:
  - `encoded_dataset.json` — tokenized input-label pairs for training
  - `vocab_mapping.json` — token-to-ID mapping
  - `label2id.json` — label-to-ID mapping
  - `label_freq.json` — label frequency counts
  - `label_priority_map.json` — priority scores for labels
  - `sheet_status.json` — status info about the Excel sheets read

### 3. `model_loader.py`
- Loads the saved Transformer model and associated vocabularies from disk.
- Instantiates the TransformerClassifier with config parameters.
- Loads saved model weights (`.pt` file).
- Provides the model in evaluation mode along with vocab and ID-label mappings.

### 4. `train_rl.py`
- Implements the Reinforcement Learning based fine-tuning for the TransformerClassifier.
- Uses a hybrid loss combining Focal Loss with Label Smoothing and REINFORCE loss.
- Applies curriculum learning, data augmentation, and exploration-exploitation scheduling.
- Rewards predictions based on correctness, class frequency, F1 score bonuses, and penalties for overprediction.
- Employs EMA (Exponential Moving Average) to stabilize training.
- Saves the best performing model as `transformer_rl.pt`.
- Tracks performance metrics such as F1 scores, reward trends, accuracy, and loss.
- Supports mixed precision training with AMP.

### 5. `data/` Directory
- Contains JSON files generated by `prepare_dataset.py` for use in training and inference:
  - `encoded_dataset.json`: tokenized input-label pairs.
  - `vocab_mapping.json`: vocabulary mapping.
  - `label2id.json`: label to ID mapping.
  - `label_freq.json`: frequency of each label in dataset.
  - `label_priority_map.json`: priority scores derived from frequencies and component criticality.
  - `sheet_status.json`: logs status of Excel sheets processed.

### 6. `tcdd-ui/frontend/frontend/src/`
- Contains `.jsx` React components forming the **web frontend UI**.
- Provides interactive web interfaces for user input, prediction display, admin panels, and possibly data visualization.
- Integrates with backend APIs to perform predictions using trained Transformer models.
  
---

## How to Use

### Prepare Dataset
1. Place your source Excel files in specified paths:
   - Maintenance records at `Vagon_cleaned.xlsx`
   - Vocabulary source at `Vagon_analysis_output.xlsx`
2. Run:
   ```bash
   python prepare_dataset.py
3.This generates JSON files in the data/ directory for training.

### Train RL Model
1.Verify that JSON dataset files are ready.

2.Run the reinforcement learning training:

     python train_rl.py

3.Training progress, metrics, and reward trends are logged.

4.Best model saved as transformer_rl.pt.

### Load Model for Inference
Use model_loader.py to load the saved model and vocab for evaluation or prediction.

### Frontend Integration
React .jsx components located in tcdd-ui/frontend/frontend/src/ provide the user interface.

These components interact with the model backend via API endpoints (to be implemented separately).

UI handles user inputs, shows predictions, and allows administrative data review.

### Key Concepts
Tokenization & Vocabulary: Converts textual categorical data into numerical tokens for model input.

Transformer Classifier: Custom transformer architecture designed to classify fault types based on encoded input tokens.

Reinforcement Learning Fine-tuning: Uses policy gradient methods (REINFORCE) combined with focal loss to boost performance, especially on rare classes.

Data Augmentation: Token dropout, swap, duplication, noise addition, and shuffle applied during training for robustness.

Curriculum Learning: Weights data samples based on difficulty and class F1 performance.

EMA Weights: Stabilize training by averaging model weights over time.

Priority Mapping: Combines component criticality and label frequency to inform model focus during training.

### Configuration Parameters
Parameter   	           Description	                              Default
MAX_LEN	                 Maximum sequence length	                    128
EMBED_SIZE	             Embedding vector size                      	384
NUM_HEADS                Number of attention heads	                  6
NUM_LAYERS             	 Number of Transformer encoder layers        	10
FF_DIM	                 Feed-forward hidden dimension	              768
DROPOUT     	           Dropout rate                               	0.15
BATCH_SIZE             	 Mini-batch size during training	            32
LR	                     Initial learning rate                      	3e-5
EPISODES	               Number of RL training episodes             	150
REWARD_CORRECT	         Reward multiplier for correct predictions	  3.0
REWARD_INCORRECT_BASE	   Base penalty for incorrect predictions	     -1.0

### Dependencies
Python 3.8+

PyTorch (with CUDA if GPU available)

numpy

pandas

scikit-learn

matplotlib

tqdm

React frontend dependencies to be installed via npm/yarn inside tcdd-ui/frontend folder.

### Notes
Make sure your Excel files follow the expected format as described in the dataset preparation.

Model training can be accelerated by GPUs.

The system supports fine-tuning with reinforcement learning to improve fault classification, especially for underrepresented classes.

Frontend React components handle UI but require backend APIs serving the trained model for full functionality.

Author
Taha Berk Erol
